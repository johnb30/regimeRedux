import re
import json
import numpy as np
import pandas as pd

from gensim import utils
from gensim.models.doc2vec import LabeledSentence
from gensim.models import Doc2Vec

from nltk.tokenize import sent_tokenize

from collections import Counter

import cPickle
from random import shuffle

NUM_SENTS = 300


class LabeledLineSentence(object):
    def __init__(self, sources, names):
        self.sources = sources
        self.names = names

    def __iter__(self):
        yr_range = 0
        for yr in xrange(1999, 2016):
            for inx, name in enumerate(self.names):
                doc_ix = yr_range + inx
                doc = self.sources[doc_ix]
                for s, sent in enumerate(doc):
                    yield LabeledSentence(sent, ['{}_{}_{}'.format(name, yr,
                                                                   s)])

    def to_array(self):
        self.sentences = []
        yr_range = 0
        for yr in xrange(1999, 2016):
            for inx, name in enumerate(self.names):
                doc_ix = yr_range + inx
                doc = self.sources[doc_ix]
                for s, sent in enumerate(doc):
                    self.sentences.append(LabeledSentence(sent,
                                                          ['{}_{}_{}'.format(name,
                                                                             yr,
                                                                             s)]))

        return self.sentences

    def sentences_perm(self):
        shuffle(self.sentences)
        return self.sentences


def clean_str(string):
    """
    Tokenization/string cleaning for all datasets except for SST.
    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py
    """
    string = re.sub(r"[^A-Za-z0-9(),!?\'\`]", " ", string)
    string = re.sub(r"\'s", " \'s", string)
    string = re.sub(r"\'ve", " \'ve", string)
    string = re.sub(r"n\'t", " n\'t", string)
    string = re.sub(r"\'re", " \'re", string)
    string = re.sub(r"\'d", " \'d", string)
    string = re.sub(r"\'ll", " \'ll", string)
    string = re.sub(r",", " , ", string)
    string = re.sub(r"!", " ! ", string)
    string = re.sub(r"\(", " \( ", string)
    string = re.sub(r"\)", " \) ", string)
    string = re.sub(r"\?", " \? ", string)
    string = re.sub(r"\s{2,}", " ", string)
    return string.strip().lower()


def build_input(text):
    #build the word count again
    #get word count dictionary

    word_counts = Counter()
    for doc in text:
        for sent in doc:
            word_counts.update(sent)

    # Mapping from index to word
    vocabulary_inv = [x[0] for x in word_counts.most_common()]
    # Mapping from word to index
    vocabulary = {x: i + 1 for i, x in enumerate(vocabulary_inv)}

    #get the final data
    x = [[[vocabulary[word] for word in sentence] for sentence in doc] for
         doc in text]
    return x, vocabulary, vocabulary_inv


def vocab_to_word2vec(fname, vocab, k=300):
    """
    Load word2vec from Mikolov
    """
    word_vecs = {}
    with open(fname, "rb") as f:
        header = f.readline()
        vocab_size, layer1_size = map(int, header.split())
        binary_len = np.dtype('float32').itemsize * layer1_size
        for line in xrange(vocab_size):
            word = []
            while True:
                ch = f.read(1)
                if ch == ' ':
                    word = ''.join(word)
                    break
                if ch != '\n':
                    word.append(ch)
            if word in vocab:
                word_vecs[word] = np.fromstring(f.read(binary_len),
                                                dtype='float32')
            else:
                f.read(binary_len)
    print str(len(word_vecs)) + " words found in word2vec."

    #add unknown words by generating random word vectors
    count_missing = 0
    for word in vocab:
        if word not in word_vecs:
            word_vecs[word] = np.random.uniform(-0.25, 0.25, k)
            count_missing += 1
    print str(count_missing) + " words not found, generated by random."
    return word_vecs


def build_word_embedding_mat(word_vecs, vocabulary_inv, k=300):
    """
    Get the word embedding matrix, of size(vocabulary_size, word_vector_size)
    ith row is the embedding of ith word in vocabulary
    """
    vocab_size = len(vocabulary_inv)
    embedding_mat = np.zeros(shape=(vocab_size + 1, k), dtype='float32')
    for idx in range(len(vocabulary_inv)):
        embedding_mat[idx + 1] = word_vecs[vocabulary_inv[idx]]
    print "Embedding matrix of size " + str(np.shape(embedding_mat))
    #initialize the first row,
    embedding_mat[0] = np.random.uniform(-0.25, 0.25, k)
    return embedding_mat

data = json.load(open('/Users/johnbeieler/regimeClassiferRedux/data/raw/StateHR_1999.json'))
countries = [x['name'] for x in data]

mmp = pd.read_csv('/Users/johnbeieler/regimeClassiferRedux/data/regimeData/mmpData_99-10.csv')
un = mmp['cname'].unique()

countries = [x.upper() for x in countries]
out = set(countries).intersection(set(un))

y = mmp[mmp['year'] == 1999]
y = y.reset_index()

use = y[y['cname'].isin(countries)]
names = use['cname'].values

hold = {}
for yr in xrange(1999, 2016):
    data = json.load(open('/Users/johnbeieler/regimeClassiferRedux/data/raw/StateHR_{}.json'.format(yr)))
    temp_hold = {yr: {}}

    for x in data:
        if x['name'].upper() in out:
            temp_hold[yr][x['name'].upper()] = x['data']

    hold.update(temp_hold)

data_list = []
y_list = []
mapping = []
for y in xrange(1999, 2016):
    for x in out:
        try:
            try:
                y_val = mmp[(mmp['cname'] == x) & (mmp['year'] == y)]['monarchy'].values[0]
                data_list.append(hold[y][x])
                y_list.append(y_val)
                mapping.append((y, x))
            except IndexError:
                data_list.append('')
                y_list.append('')
                mapping.append('')
        except KeyError:
            data_list.append('')
            y_list.append('')
            mapping.append('')
data_list = [x.replace('\n', '') for x in data_list]

text = [sent_tokenize(d) for d in data_list]
text = [[clean_str(s).split(" ") for s in j] for j in text]

x, vocabulary, vocabulary_inv = build_input(text)
word2vec = vocab_to_word2vec('/Users/johnbeieler/GoogleNews-vectors-negative300.bin', vocabulary)
embedding_mat = build_word_embedding_mat(word2vec, vocabulary_inv)

sentences = LabeledLineSentence(text, names)
model = Doc2Vec.load_word2vec_format('/Users/johnbeieler/GoogleNews-vectors-negative300.bin',
                                     binary=True)
#model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5,
#                workers=8)
model.build_vocab(sentences.to_array())
for epoch in range(10):
    model.train(sentences.sentences_perm())

model.save('HR_docs.d2v')

data = np.zeros((len(data_list), NUM_SENTS, 100))
for ix, x in enumerate(mapping):
    for w in xrange(NUM_SENTS):
        if x:
            try:
                data[ix, w, :] = model.docvecs['{}_{}_{}'.format(x[1], x[0],
                                                                 w)]
            except KeyError:
                pass

cPickle.dump([data, np.toarray(y)], open('HR_monarchy_data.p', 'wb'))
